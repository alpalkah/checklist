{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import checklist\n",
    "from checklist.editor import Editor\n",
    "from checklist.perturb import Perturb\n",
    "from checklist.test_types import MFT, INV, DIR\n",
    "from checklist.test_suite import TestSuite\n",
    "from checklist.expect import Expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you read the [paper](https://homes.cs.washington.edu/~marcotcr/acl20_checklist.pdf), you know that CheckList is more than this package, it's also a process.  \n",
    "This tutorial is a short version of that process, but you should really read the paper if you haven't :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task and Model: QQP, BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this tutorial, we'll use Quora Question Pair as an example, with [a finetuned BERT model hosted by Textattack](https://huggingface.co/textattack/bert-base-uncased-QQP).\n",
    "**Please note that this is not the model reported in the paper -- we finetuned that model locally.** \n",
    "Here, we instead use a model that is available online (loaded through [Huggingface Pipeline](https://huggingface.co/transformers/main_classes/pipelines.html)), so that you can easily follow the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model and spacy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import spacy\n",
    "import numpy as np\n",
    "processor = spacy.load('en_core_web_sm')\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "model_name = \"textattack/bert-base-uncased-QQP\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "# sentiment analysis is a general name in Huggingface to load the pipeline for text classification tasks.\n",
    "# set device=-1 if you don't have a gpu\n",
    "pipe = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, framework=\"pt\", device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset. First, install the [NLP dataset library](https://huggingface.co/nlp/quicktour.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nlp\n",
      "  Downloading nlp-0.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: numpy in e:\\msai\\nlp\\nlp\\.venv\\lib\\site-packages (from nlp) (2.3.4)\n",
      "Requirement already satisfied: pyarrow>=0.16.0 in e:\\msai\\nlp\\nlp\\.venv\\lib\\site-packages (from nlp) (22.0.0)\n",
      "Requirement already satisfied: dill in e:\\msai\\nlp\\nlp\\.venv\\lib\\site-packages (from nlp) (0.4.0)\n",
      "Requirement already satisfied: pandas in e:\\msai\\nlp\\nlp\\.venv\\lib\\site-packages (from nlp) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in e:\\msai\\nlp\\nlp\\.venv\\lib\\site-packages (from nlp) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in e:\\msai\\nlp\\nlp\\.venv\\lib\\site-packages (from nlp) (4.67.1)\n",
      "Requirement already satisfied: filelock in e:\\msai\\nlp\\nlp\\.venv\\lib\\site-packages (from nlp) (3.20.0)\n",
      "Requirement already satisfied: xxhash in e:\\msai\\nlp\\nlp\\.venv\\lib\\site-packages (from nlp) (3.6.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\msai\\nlp\\nlp\\.venv\\lib\\site-packages (from requests>=2.19.0->nlp) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\msai\\nlp\\nlp\\.venv\\lib\\site-packages (from requests>=2.19.0->nlp) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\msai\\nlp\\nlp\\.venv\\lib\\site-packages (from requests>=2.19.0->nlp) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\msai\\nlp\\nlp\\.venv\\lib\\site-packages (from requests>=2.19.0->nlp) (2025.10.5)\n",
      "Requirement already satisfied: colorama in e:\\msai\\nlp\\nlp\\.venv\\lib\\site-packages (from tqdm>=4.27->nlp) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\msai\\nlp\\nlp\\.venv\\lib\\site-packages (from pandas->nlp) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\msai\\nlp\\nlp\\.venv\\lib\\site-packages (from pandas->nlp) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\msai\\nlp\\nlp\\.venv\\lib\\site-packages (from pandas->nlp) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in e:\\msai\\nlp\\nlp\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->nlp) (1.17.0)\n",
      "Downloading nlp-0.4.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 12.8 MB/s  0:00:00\n",
      "Installing collected packages: nlp\n",
      "Successfully installed nlp-0.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a91d9d3c4a064eef86cd2cc1240ae2e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbad2a09f29946cd9ce8db901cc85f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/33.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b775a3572f2147e897b7356563947b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/3.73M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc27617e404b44ff8cd5824eb0c3d795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/36.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c213c45228f044049f88f939d570f787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/363846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be4bf0cc91ef47f3821b1b194eaf290e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/40430 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3009c85795842158e663e6b0ae062cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/390965 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'question1': 'Why are African-Americans so beautiful?',\n",
       " 'question2': 'Why are hispanics so beautiful?',\n",
       " 'label': 0,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "qqp_data = load_dataset('glue', 'qqp', split='validation')\n",
    "all_questions = set()\n",
    "q1s = [d[\"question1\"] for d in qqp_data]\n",
    "q2s = [d[\"question2\"] for d in qqp_data]\n",
    "labels = np.array([d[\"label\"] for d in qqp_data]).astype(int)\n",
    "\n",
    "qs = list(zip(q1s, q2s))\n",
    "qqp_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess all the questions with spacy. This may take sometime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count of unique questions: 73324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73324it [02:00, 607.50it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "all_questions.update(set(q1s))\n",
    "all_questions.update(set(q2s))\n",
    "print(f\"Total count of unique questions: {len(all_questions)}\")\n",
    "processed_qs = list(tqdm(processor.pipe(all_questions, batch_size=64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_map = {q: processed_q for (q, processed_q) in zip(all_questions, processed_qs)}\n",
    "parsed_qs = [(spacy_map[q[0]], spacy_map[q[1]]) for q in qs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top-Down approach: the CheckList matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capabilities x Test Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In tutorial #3, we talked about specific test types.  \n",
    "In order to guide test ideation, it's useful to think of CheckList as a matrix of Capabilities x Test Types.  \n",
    "*Capabilities* refers to general-purpose linguistic capabilities, which manifest in one way or another in almost any NLP application.   \n",
    "We suggest that anyone CheckListing a model go through *at least* the following capabilities, trying to create MFTs, INVs, and DIRs for each if possible.\n",
    "1. **Vocabulary + POS:** important words or groups of words (by part-of-speech) for the task\n",
    "2. **Taxonomy**: synonyms, antonyms, word categories, etc\n",
    "3. **Robustness**: to typos, irrelevant additions, contractions, etc\n",
    "4. **Named Entity Recognition (NER)**: person names, locations, numbers, etc\n",
    "5. **Fairness**\n",
    "6. **Temporal understanding**: understanding order of events and how they impact the task\n",
    "7. **Negation**\n",
    "8. **Coreference** \n",
    "9. **Semantic Role Labeling (SRL)**: understanding roles such as agent, object, passive/active, etc\n",
    "10. **Logic**: symmetry, consistency, conjunctions, disjunctions, etc\n",
    "\n",
    "Notice that we are framing this as very top-down approach: you start with a list of capabilities and try to think of what kinds of tests can be created, based on the three test types. We'll talk about how to incorporate some bottom-up thinking later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't try to create tests for **all** of these capabilities (but we do have notebooks with tests for all of them in the repo), just one as an example. \n",
    "Anyway, let's create a test suite (used to save and aggregate tests):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = TestSuite()\n",
    "editor = Editor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capability: NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the NER capability.  \n",
    "How do named entities impact duplicate question detection? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MFT\n",
    "It seems that the model should at least be able to distinguish questions about different people as non-duplicates.   \n",
    "Let's write an MFT where we have two people that have the same last name, but different first names.  \n",
    "Instead of running the test now, we'll add it to the suite and run all tests later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Is Fiona Taylor fired?', 'Is Ray Taylor fired?')\n",
      "('Is Kathy Gordon poisoned?', 'Is Edwin Gordon poisoned?')\n"
     ]
    }
   ],
   "source": [
    "t = editor.template((\n",
    "    'Is {first_name} {last_name} {mask}?',\n",
    "    'Is {first_name2} {last_name} {mask}?',\n",
    "    ),\n",
    "    remove_duplicates=True, \n",
    "    nsamples=300)\n",
    "test = MFT(**t, labels=0, name='same adjectives, different people', capability = 'NER',\n",
    "          description='Different first name, same adjective and last name')\n",
    "suite.add(test)\n",
    "print(t.data[0])\n",
    "print(t.data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INV\n",
    "If you have two questions with the same named entity, changing the entity on both should not change whether the questions are duplicates or not.  \n",
    "Let's write an INV for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are dealing with pairs of questions, we have to write a wrapper to make sure the same name is changed on both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_name_on_both(qs):\n",
    "    q1, q2 = qs\n",
    "    c1 = Perturb.change_names(q1, seed=1, meta=True)\n",
    "    c2 = Perturb.change_names(q2, seed=1, meta=True)\n",
    "    if not c1 or not c2:\n",
    "        return\n",
    "    # separating out examples and meta. Meta has tuples (a, b), where name 'a' was changed to 'b'\n",
    "    c1, m1 = c1\n",
    "    c2, m2 = c2\n",
    "    # Only include examples where the same name was changed on both questions\n",
    "    return [(q1, q2) for q1, q2, m1, m2 in zip(c1, c2, m1, m2) if m1 == m2][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('What was it like debating against Ted Cruz when he was at Princeton?', 'Did Ted Cruz debate while at Harvard Law?')\n",
      "('What was it like debating against Michael Brooks when he was at Princeton?', 'Did Michael Brooks debate while at Harvard Law?')\n",
      "('What was it like debating against Christopher Cox when he was at Princeton?', 'Did Christopher Cox debate while at Harvard Law?')\n"
     ]
    }
   ],
   "source": [
    "t = Perturb.perturb(parsed_qs, change_name_on_both, nsamples=200)\n",
    "test = INV(**t, name='Change same name in both questions', capability='NER',\n",
    "          description='')\n",
    "# test.run(new_pp)\n",
    "# test.summary(3)\n",
    "suite.add(test)\n",
    "print(t.data[0][0])\n",
    "print(t.data[0][1])\n",
    "print(t.data[0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIR\n",
    "Conversely, if an entity is present on a pair the model predicts as a duplicate and we change it to something else on *only one* of the sentences, the prediction should change to non-duplicate.  \n",
    "Let's write this as a DIR test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_name_on_one(qs):\n",
    "    q1, q2 = qs\n",
    "    c1 = Perturb.change_names(q1, seed=1, meta=True)\n",
    "    c2 = Perturb.change_names(q2, seed=1, meta=True)\n",
    "    if not c1 or not c2:\n",
    "        return\n",
    "    c1, m1 = c1\n",
    "    c2, m2 = c2\n",
    "    ret = []\n",
    "    ret.extend([(q1_, str(q2)) for q1_, m1_ in zip(c1, m1) if m1_[0] in str(q2)])\n",
    "    ret.extend([(str(q1), q2_) for q2_, m2_ in zip(c2, m2) if m2_[0] in str(q1)])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll write an expectation function in two steps.  \n",
    "First, we want the prediction to be 0.  \n",
    "Second, we only want to include examples where the original prediction is one. We do this with a slice wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "expect_fn = Expect.eq(0)\n",
    "expect_fn = Expect.slice_orig(expect_fn, lambda orig, *args: orig == 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put it all together into a test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Who do you think will win, Trump or Hillary?', 'Who is going to win, Trump or Hillary?')\n",
      "('Who do you think will win, Trump or Kayla?', 'Who is going to win, Trump or Hillary?')\n",
      "('Who do you think will win, Trump or Kimberly?', 'Who is going to win, Trump or Hillary?')\n"
     ]
    }
   ],
   "source": [
    "t = Perturb.perturb(parsed_qs, change_name_on_one, nsamples=200)\n",
    "name = 'Change name in one of the questions'\n",
    "desc = 'Take pairs that are originally predicted as duplicates, change name in one of them and expect new prediction to be non-duplicate'\n",
    "test = DIR(**t, expect=expect_fn, name=name, description=desc, capability='NER')\n",
    "suite.add(test)\n",
    "print(t.data[0][0])\n",
    "print(t.data[0][1])\n",
    "print(t.data[0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These examples illustrate how thinking through the matrix can help test ideation. We now turn to a bottom up approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bottom up approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approach, we look at specific examples (from the validation dataset or elsewhere) and try to generalize them into MFTs, INVs or DIRs, placing them into a specific capability.  \n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Which company should I join as a fresher, TCS or Virtusa?',\n",
       " 'Is it a good decision to join Tcs as a fresher?')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(14)\n",
    "i = np.random.choice(len(qs))\n",
    "qs[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good example, in which a question asks about a comparison between two options, while the other question asks about a single option.  \n",
    "While they are not duplicates, it is possible that models would get confused here. I think this test fits into the Vocabulary+POS capability (it's not crucial for us to be completely precise about where a test fits).  \n",
    "Let's try to create an MFT out of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apple, Google, Facebook, This, Microsoft, Amazon, Uber, It, Intel, Samsung, Netflix, Tesla, Twitter, LinkedIn, Oracle, Target, Snap, Disney, AMD, Bloomberg, Sony, That, Wikipedia, China, Here, Fox, this, HP, FB, YouTube, Reddit, Ford, Pinterest, Harris, MIT, GE, CBS, Dialog, Square, Orange'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join(editor.suggest('{mask} is a large tech company.')[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = ['Apple', 'Google', 'Facebook', 'Microsoft', 'Amazon', 'Uber', 'Intel', 'Samsung', 'Netflix', 'Tesla', 'LinkedIn', 'Oracle', 'Target', 'Snap', 'Disney', 'AMD', 'Sony', 'Reddit', 'Youtube']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'member, customer, shareholder, volunteer, contributor, developer, professional, competitor, writer, student, contractor, manager, subscriber, rookie, CEO, pro, director, Developer, reader, sponsor, client, worker, consultant, person, Guardian, seller, buyer, user, Beta, journalist'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join(editor.suggest('Should I join {company} as a {mask}?', company=companies)[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = ['developer', 'contributor', 'freshman', 'college grad', 'volunteer', 'writer', 'contractor', 'consultant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template((\n",
    "       'Which company should I join as a {role}, {company1} or {company2}?',\n",
    "       'Should I join {company1} as a {role}?',\n",
    "   ),\n",
    "    company=companies,\n",
    "    role=role,\n",
    "    remove_duplicates=True,\n",
    "    nsamples=100,\n",
    ")\n",
    "print(t.data[0])\n",
    "print(t.data[1])\n",
    "print(t.data[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've replicated the original example, but we can generalize it a bit to other comparisons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "', '.join([str(x) for x in editor.suggest('Will Google\\'s {mask} {mask}?')][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t += editor.template((\n",
    "       'Which company\\'s {fverb[0]} will {fverb[1]} {comp}, {company1} or {company2}?',\n",
    "       'Will {company1}\\'s {fverb[0]} {fverb[1]}?',\n",
    "   ),\n",
    "    company=companies,\n",
    "    comp=['most', 'least', 'sooner', 'later'],\n",
    "    fverb=[('stock', 'rise'), ('CEO', 'quit'), ('board', 'resign'), ('stock', 'fall'), ('effort', 'succeed'), ('strategy', 'work'), ('plan', 'work'), ('gamble', 'work'), ('focus', 'change'), ('intentions', 'change')],\n",
    "    nsamples=300,\n",
    "    remove_duplicates=True,\n",
    ")\n",
    "print(t.data[-1])\n",
    "print(t.data[-2])\n",
    "print(t.data[-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MFT(**t, labels=0, name='Comparison between two entities is not the same as asking about one', capability = 'Vocabulary',\n",
    "          description='')\n",
    "suite.add(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the suite, seeing results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running the prediction, the Huggingface pipeline returns a dict with predicted label and probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = ('Which company should I join as a freshman, Google or Facebook?', 'Should I join Google as a freshman?')\n",
    "# example = ('Which company should I join as a freshman, Google or Facebook?', 'Facebook is a great company')\n",
    "pipe([example])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a simple wrapper to make the output compatible with CheckList:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_and_conf(data):\n",
    "    raw_preds = pipe(data)\n",
    "    preds = np.array([ int(p[\"label\"][-1]) for p in raw_preds])\n",
    "    pp = np.array([[p[\"score\"], 1-p[\"score\"]] if int(p[\"label\"][-1]) == 0 else [1-p[\"score\"], p[\"score\"]] for p in raw_preds])\n",
    "    return preds, pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite.run(pred_and_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a (text) summary of the results by calling `suite.summary()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if we're using jupyter, we can use a nifty visualization that has all of the tests we created in a matrix.  \n",
    "You can navigate the matrix and see results for individual tests (*The screenshot below is based on our locally finetuned model, so the numbers may not match with your results.*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from IPython.display import HTML, Image\n",
    "# with open('visual_table_summary.gif','rb') as f:\n",
    "#     display(Image(data=f.read(), format='png'))\n",
    "suite.visual_summary_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: testing Taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a few additional tests for the Taxonomy capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "x = editor.suggest('How can I become more {mask}?')\n",
    "x += editor.suggest('How can I become less {mask}?')\n",
    "for a in set(x):\n",
    "    e = editor.synonyms('How can I become {moreless} %s?' % a, a, moreless=['more', 'less'])\n",
    "    if e:\n",
    "#         print(a, [b[0][0] for b in e] )\n",
    "        tmp.append([a] + e)\n",
    "#         opps.append((a, e[0][0][0]))\n",
    "print(', '.join([str(tuple(x)) for x in tmp][:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of all of those, let's pick a few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [ ('spiritual', 'religious'), ('angry', 'furious'), ('organized', 'organised'),\n",
    "            ('vocal', 'outspoken'), ('grateful', 'thankful'), ('intelligent', 'smart'),\n",
    "            ('humble', 'modest'), ('courageous', 'brave'), ('happy', 'joyful'), ('scared', 'frightened'),\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these, we can create a simple MFT, where we expect the model to recognize these synonyms.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template(\n",
    "    (\n",
    "    'How can I become {moreless} {x[0]}?',\n",
    "    'How can I become {moreless} {x[1]}?',\n",
    "    ),\n",
    "    x=synonyms,\n",
    "    moreless=['more', 'less'],\n",
    "    remove_duplicates=True, \n",
    "    nsamples=200)\n",
    "name = 'How can I become more {synonym}?' \n",
    "desc = 'different (simple) templates where words are replaced with their synonyms'\n",
    "test = MFT(**t, labels=1, name=name, capability = 'Taxonomy',\n",
    "          description=desc)\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same with antonyms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opps = []\n",
    "x = editor.suggest('How can I become more {mask}?')\n",
    "x += editor.suggest('How can I become less {mask}?')\n",
    "for a in set(x):\n",
    "    e = editor.antonyms('How can I become {moreless} %s?' % a, a, moreless=['more', 'less'])\n",
    "    if e:\n",
    "#         print(a, [b[0][0] for b in e] )\n",
    "        opps.append([a] + e)\n",
    "#         opps.append((a, e[0][0][0]))\n",
    "print(','.join([str(tuple(x)) for x in opps]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antonyms = [('progressive', 'conservative'),('religious', 'secular'),('positive', 'negative'),('defensive', 'offensive'),('rude',  'polite'),('optimistic', 'pessimistic'),('stupid', 'smart'),('negative', 'positive'),('unhappy', 'happy'),('active', 'passive'),('impatient', 'patient'),('powerless', 'powerful'),('visible', 'invisible'),('fat', 'thin'),('bad', 'good'),('cautious', 'brave'), ('hopeful', 'hopeless'),('insecure', 'secure'),('humble', 'proud'),('passive', 'active'),('dependent', 'independent'),('pessimistic', 'optimistic'),('irresponsible', 'responsible'),('courageous', 'fearful')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template([(\n",
    "    'How can I become more {x[0]}?',\n",
    "    'How can I become less {x[1]}?',\n",
    "    ),\n",
    "    (\n",
    "    'How can I become less {x[0]}?',\n",
    "    'How can I become more {x[1]}?',\n",
    "    )],\n",
    "    unroll=True,\n",
    "    x=antonyms,\n",
    "    remove_duplicates=True, \n",
    "    nsamples=300)\n",
    "name = 'How can I become more X = How can I become less antonym(X)' \n",
    "desc = ''\n",
    "test = MFT(**t, labels=1, name=name, capability = 'Taxonomy',\n",
    "          description=desc)\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be easy to turn the synonym one into an INV as well (we do this in another notebook), but let's end here after we run the suite again and see new results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite.run(pred_and_conf, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite.visual_summary_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
